{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "described-start",
   "metadata": {},
   "source": [
    "### Confirm we can use a GPU to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-origin",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data/kuze_data/evaluations_per_ans_with_taxonomy_ids_PPL.csv\"\n",
    "factorized_taxonomies = \"data/kuze_data/factorized_math_taxonomies.csv\"\n",
    "factorized_students = \"data/kuze_data/factorized_student_ids.csv\"\n",
    "verbose = 1\n",
    "best_model_weights = \"weights/bestmodel\"\n",
    "log_dir = \"logs\"\n",
    "optimizer = \"adam\"\n",
    "lstm_units = 200\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "dropout_rate = 0.3\n",
    "test_fraction = 0.2\n",
    "validation_fraction = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-muscle",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/grenouille/Documents/jenga/final_project/code/kuze_dkt_imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepkt import deepkt, data_util, metrics\n",
    "\n",
    "dataset, length, nb_features, nb_taxonomies = data_util.load_dataset(data, factorized_taxonomies, factorized_students, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_set, test_set, val_set = data_util.split_dataset(dataset=dataset, total_size=length, test_fraction=test_fraction, val_fraction=validation_fraction)\n",
    "\n",
    "set_size = length * batch_size\n",
    "\n",
    "test_set_size = (set_size * test_fraction)\n",
    "\n",
    "val_set_size = (set_size - test_set_size) * validation_fraction\n",
    "\n",
    "train_set_size = set_size - test_set_size - val_set_size\n",
    "\n",
    "print(\"============== Data Summary ==============\")\n",
    "print(\"Total number of students: %d\" % set_size)\n",
    "print(\"Training set size: %d\" % train_set_size)\n",
    "print(\"Validation set size: %d\" % val_set_size)\n",
    "print(\"Testing set size: %d\" % test_set_size)\n",
    "print(\"Number of skills: %d\" % nb_taxonomies)\n",
    "print(\"Number of features in the input: %d\" % nb_features)\n",
    "print(\"========================================= \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-springer",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = deepkt.DKTModel(\n",
    "        nb_features=nb_features,\n",
    "        nb_taxonomies=nb_taxonomies,\n",
    "        hidden_units=lstm_units,\n",
    "        dropout_rate=dropout_rate)\n",
    "\n",
    "student_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\n",
    "            metrics.BinaryAccuracy(),\n",
    "            metrics.AUC(),\n",
    "            metrics.Precision(),\n",
    "            metrics.Recall()\n",
    "        ])\n",
    "\n",
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-animation",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = student_model.fit(\n",
    "    dataset=train_set,\n",
    "    epochs=epochs,\n",
    "    verbose=verbose,\n",
    "    validation_data=val_set,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n",
    "        tf.keras.callbacks.ModelCheckpoint(best_model_weights, save_best_only=True, save_weights_only=True),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-throat",
   "metadata": {},
   "source": [
    "### Load the model with the best validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.load_weights(best_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-english",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = student_model.evaluate(test_set, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.save('student_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-museum",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_prediction(dataframe):\n",
    "    seq = dataframe.groupby('student_id').apply(\n",
    "        lambda r: (\n",
    "            r['factorized_student_id'],\n",
    "            r['factorized_taxonomy_id']\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator=lambda: seq,\n",
    "        output_types=(tf.int32, tf.int32)\n",
    "    )\n",
    "\n",
    "    # Add 1 since indexing starts from 0\n",
    "    student_depth = int(students['factorized_student_id'].max() + 1)\n",
    "    taxonomy_depth = int(taxonomies['factorized_taxonomy_code'].max() + 1)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda factorized_student_id, factorized_taxonomy_code: (\n",
    "            tf.one_hot(factorized_student_id, depth=student_depth),\n",
    "            tf.one_hot(factorized_taxonomy_code, depth=taxonomy_depth)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size=64,\n",
    "        padding_values=(\n",
    "            tf.constant(-1, dtype=tf.float32),\n",
    "            tf.constant(-1, dtype=tf.float32)),\n",
    "        padded_shapes=([None, None], [None, None])\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_student_data(dataset):\n",
    "    \"\"\"Preprocess the tensorflow Dataset type used for prediction.\n",
    "    The first item in the dataset corresponds to the student information.\n",
    "    Dimensions:\n",
    "        -> batch size\n",
    "        -> number of elements per batch\n",
    "        -> one-hot encoded data (number of students)\n",
    "    We want to get the categorical student_id from the one-hot encoding.\n",
    "    Return a list containing the categorical student_id\n",
    "    \"\"\"\n",
    "    student_id_list = []\n",
    "    student_val_list = []\n",
    "    for i in range(len(dataset[0][0])):\n",
    "        for j in range(len(dataset[0][0][i])):\n",
    "            array = dataset[0][0][i][j]\n",
    "            idx = np.argmax(array)\n",
    "            student_id_list.append(idx)\n",
    "            student_val_list.append(array[idx].numpy())\n",
    "    return student_id_list, student_val_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_taxonomy_data(dataset):\n",
    "    \"\"\"Preprocess the tensorflow Dataset type used for prediction.\n",
    "    The second item in the dataset corresponds to the taxonomy information.\n",
    "    Dimensions:\n",
    "        -> batch size\n",
    "        -> number of elements per batch\n",
    "        -> one-hot encoded data (number of students)\n",
    "    We want to get the categorical student_id from the one-hot encoding.\n",
    "    Return a list containing the categorical student_id\n",
    "    \"\"\"\n",
    "    taxonomy_id_list = []\n",
    "    taxonomy_val_list = []\n",
    "    for i in range(len(dataset[0][1])):\n",
    "        for j in range(len(dataset[0][1][i])):\n",
    "            array = dataset[0][1][i][j]\n",
    "            idx = np.argmax(array)\n",
    "            taxonomy_id_list.append(idx)\n",
    "            taxonomy_val_list.append(array[idx].numpy())\n",
    "    return taxonomy_id_list, taxonomy_val_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_prediction_data(predictions):\n",
    "    \"\"\"Expose relevant predictions from the predictions array.\n",
    "    Dimensions:\n",
    "        -> batch size\n",
    "        -> number of elements per batch\n",
    "        -> one-hot encoded data (number of taxonomies)\n",
    "    Return one-hot encoded arrays sequentially ordered.\n",
    "    \"\"\"\n",
    "    prediction_array_list = []\n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            prediction_array_list.append(predictions[i][j])\n",
    "    return prediction_array_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction_data(predictions, taxonomy_id_list):\n",
    "    \"\"\"Get the predicted value for a taxonomy.\n",
    "    Predictions is a list of arrays containing predictions for all\n",
    "    taxonomies.\n",
    "    The arrays within the list are sequentially ordered.\n",
    "    To get the relevant array we index into the list of arrays\n",
    "    with the index of the taxonomy_id of current interest within\n",
    "    the taxonomy_id_list\n",
    "    To get the prediction for the taxonomy of interest, we index\n",
    "    into the array with the taxonomy_id.\n",
    "    Return a list of predicted values.\n",
    "    Length should be equal to that of taxonomy_id_list.\n",
    "    \"\"\"\n",
    "    taxonomy_predictions = []\n",
    "    for idx, taxonomy_code in enumerate(taxonomy_id_list):\n",
    "        prediction_array = predictions[idx]\n",
    "        taxonomy_predictions.append(prediction_array[taxonomy_code])\n",
    "    assert len(taxonomy_predictions) == len(taxonomy_id_list)\n",
    "    return taxonomy_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_prediction_preprocessing(dataset, predictions):\n",
    "    \"\"\"Process the dataset and predictions into a pandas DataFrame.\n",
    "    We want to take the input dataset and match it to the corresponding\n",
    "    predictions.\n",
    "    The dataset has paddings in order to conform to expected dimensions.\n",
    "    Padding value is -1 and that is where the student_val_list and\n",
    "    taxonomy_val_list come in handy.\n",
    "    Any values with -1 in those 2 lists corresponds to a padding value\n",
    "    and can therefore be dropped\"\"\"\n",
    "    # convert the dataset into a list for easy access and manipulation\n",
    "    dataset = list(dataset)\n",
    "    student_id_list, student_val_list = process_student_data(dataset)\n",
    "    taxonomy_id_list, taxonomy_val_list = process_taxonomy_data(dataset)\n",
    "    preprocessed_prediction_list = preprocess_prediction_data(predictions)\n",
    "    taxonomy_predictions = process_prediction_data(\n",
    "            preprocessed_prediction_list, taxonomy_id_list)\n",
    "\n",
    "    # round off all values in taxonomy_predictions to 2 decimal places\n",
    "    # for readability\n",
    "    taxonomy_predictions = [round(i, 4) for i in taxonomy_predictions]\n",
    "\n",
    "    column_names = ['factorized_student_id', 'one-hot_student_value', 'factorized_taxonomy_id',\n",
    "                    'one-hot_taxonomy_value', 'prediction']\n",
    "    prediction_df = pd.DataFrame(list(zip(student_id_list,\n",
    "                                    student_val_list,\n",
    "                                    taxonomy_id_list,\n",
    "                                    taxonomy_val_list,\n",
    "                                    taxonomy_predictions)),\n",
    "                           columns=column_names)\n",
    "\n",
    "    # remove padding values from students and taxonomies\n",
    "    prediction_df = prediction_df[prediction_df['one-hot_student_value'] != -1]\n",
    "    prediction_df = prediction_df[prediction_df['one-hot_taxonomy_value'] != -1]\n",
    "\n",
    "    # if the value of the prediction is greater than or equal to 0.5\n",
    "    # the predicted answer should be 1 else 0\n",
    "    # astype('int') converts a boolean value to an integer True == 1, False == 0\n",
    "    prediction_df['predicted_answer'] = prediction_df['prediction'].ge(0.5).astype('int')\n",
    "    prediction_df['predicted_answer'] = prediction_df['predicted_answer'].astype('int')\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/kuze_data/predictor_evaluations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = data[data['subject'] == 'math']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomies = pd.read_csv(factorized_taxonomies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.read_csv(factorized_students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data['factorized_taxonomy_id'] = prediction_data['taxonomy_id_0'].map(\n",
    "    taxonomies.set_index('taxonomy_id_0')['factorized_taxonomy_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data['factorized_student_id'] = prediction_data['student_id'].map(\n",
    "    students.set_index('student_id')['factorized_student_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = prediction_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to limitations in dimensionality we want each dataframe we predict on to have\n",
    "# 95 items\n",
    "no_of_dataframes = shape // 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into n number of dataframes each with at least 95 rows\n",
    "partitions = np.array_split(prediction_data, no_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry out prediction on a partition of the predicted data and append\n",
    "# the returned dataframe to a list\n",
    "predicted_partitions = []\n",
    "for df in partitions:\n",
    "    dataset = preprocess_for_prediction(df)\n",
    "    predictions = student_model.predict(dataset)\n",
    "    prediction_data = post_prediction_preprocessing(dataset, predictions)\n",
    "    predicted_partitions.append(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = pd.concat(partitions, ignore_index=True)\n",
    "predictions = pd.concat(predicted_partitions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert prediction_data.shape[0] == predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = prediction_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_predictions = predictions['predicted_answer'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(rows):\n",
    "    prediction_data.at[i, 'answer_selection_prediction'] = answer_predictions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregated_evaluation_performance(dataframe, with_preds=True):\n",
    "    # Group data by student and evaluation id and calculate actual and predicted\n",
    "    # performance on questions\n",
    "    eval_id = []\n",
    "    student = []\n",
    "    total_questions = []\n",
    "    actual_performance = []\n",
    "    predicted_performance = []\n",
    "    date_of_evaluation = []\n",
    "    subject = []\n",
    "    student_full_name = []\n",
    "    class_name = []\n",
    "    class_grade = []\n",
    "    school_name = []\n",
    "\n",
    "    grouped_data = dataframe.groupby(['evaluation_id', 'student_id'])\n",
    "\n",
    "    for item in grouped_data:\n",
    "        evaluation_id, student_id = item[0]\n",
    "        data = item[1]\n",
    "        actual = data['answer_selection_correct'].value_counts()\n",
    "        total_nu_questions = actual.sum()\n",
    "        first_name = data['student_first_name'].unique()[0]\n",
    "        last_name = data['student_last_name'].unique()[0]\n",
    "\n",
    "        if first_name is np.nan:\n",
    "            first_name = ''\n",
    "        if last_name is np.nan:\n",
    "            last_name = ''\n",
    "\n",
    "        full_name = first_name + ' ' + last_name\n",
    "\n",
    "        try:\n",
    "            actual_correct = actual[1]\n",
    "        except KeyError:\n",
    "            # if a KeyError occurs it means the student got all of the\n",
    "            # questions in that evaluation wrong\n",
    "            actual_correct = 0\n",
    "        actual_perc = int((actual_correct / total_nu_questions) * 100)\n",
    "\n",
    "        if with_preds:  # if prediction data is included\n",
    "            predicted = data['answer_selection_prediction'].astype('int').value_counts()\n",
    "\n",
    "            # ensure acual no of questions done matches no of questions predicted\n",
    "            assert actual.sum() == predicted.sum()\n",
    "\n",
    "            predicted_correct = predicted[1]\n",
    "            predicted_perc = int((predicted_correct/ total_nu_questions) * 100)\n",
    "            predicted_performance.append(predicted_perc)\n",
    "        else:\n",
    "            predicted_performance.append(0)\n",
    "\n",
    "        eval_id.append(evaluation_id)\n",
    "        student.append(student_id)\n",
    "        total_questions.append(total_nu_questions)\n",
    "        actual_performance.append(actual_perc)\n",
    "        date_of_evaluation.append(data['date_of_evaluation'].unique()[0].date())\n",
    "        subject.append(data['subject'].unique()[0])\n",
    "        student_full_name.append(full_name)\n",
    "        class_name.append(data['class_name'].unique()[0])\n",
    "        class_grade.append(data['class_grade'].unique()[0])\n",
    "        school_name.append(data['school_name'].unique()[0])\n",
    "\n",
    "    column_names = ['evaluation_id', 'student_id', 'total_number_of_questions', 'actual_performance (%)',\n",
    "                    'predicted_performance (%)', 'date_of_evaluation', 'subject', 'student_full_name',\n",
    "                    'class_name', 'class_grade', 'school_name']\n",
    "    performance_df = pd.DataFrame(list(zip(eval_id,\n",
    "                                           student,\n",
    "                                           total_questions,\n",
    "                                           actual_performance,\n",
    "                                           predicted_performance,\n",
    "                                           date_of_evaluation,\n",
    "                                           subject,\n",
    "                                           student_full_name,\n",
    "                                           class_name,\n",
    "                                           class_grade,\n",
    "                                           school_name)),\n",
    "                                 columns=column_names)\n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (ds_evaluation_per_ans_sci_prediction_df['date_of_evaluation'] < '2021-07-01')\n",
    "\n",
    "training_data = ds_evaluation_per_ans_sci_prediction_df.loc[mask]\n",
    "\n",
    "# training_data.dropna(subset=['answer_selection_correct'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_training_data = get_aggregated_evaluation_performance(training_data, with_preds=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_data = get_aggregated_evaluation_performance(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_performance_data = pd.concat([aggregated_training_data, performance_data], ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
